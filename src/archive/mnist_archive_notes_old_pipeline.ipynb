{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dataset ###\n",
    "\n",
    "#  tfds.list_builders() # List all registered datasets\n",
    "\n",
    "# Load datasets from tf datasets. Also saves data locally to C:\\Users\\*\\tensorflow_datasets the first time dataset is loaded\n",
    "# as_supervised=True loads in a tuple with structure: [input, target]\n",
    "# with_info=True will also return another tuple containing info on version, features, # samples of the dataset, etc.\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_dir='C:\\\\Users\\\\alowe\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-process the data ###\n",
    "\n",
    "# Extract traing and test data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] # 60k and 10k examples, respectively\n",
    "\n",
    "# Establish an amount to split for validation dataset, and cast it as a tensor of that size\n",
    "num_val_samples = 0.1 * mnist_info.splits['train'].num_examples # 10% (arbitrary) of 60k train dataset will be for validation\n",
    "num_val_samples = tf.cast(num_val_samples, tf.int64) # Convert/cast to integer (avoid having a float)\n",
    "\n",
    "# Do the same as the above step but for a tensor for the test dataset\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# Scale data so that inputs are b/w 0 and 1 (integers b/w 0-255 to floats b/w 0-1)\n",
    "def scale_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# Apply the above function using tf's inbuilt dataset.map(*function*), which applies a custom transformation to a given dataset.\n",
    "# Note: This .map(*function*) method can only apply a function that takes in (input, label) and returns (input, label).\n",
    "scaled_train_and_val_data = mnist_train.map(scale_image)\n",
    "test_data = mnist_test.map(scale_image)\n",
    "\n",
    "# Shuffle the data\n",
    "BUFFER_SIZE = 10000 # When dealing with enormous datasets, we can't shuffle all data at once (b/c of system memory), so we'll shuffle 10k at a time\n",
    "shuffled_train_and_val_data = scaled_train_and_val_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Partition data - Now we can \"take\" validation data from the shuffled train+val data and \"skip\" that same data for the test data\n",
    "val_data = shuffled_train_and_val_data.take(num_val_samples)\n",
    "train_data = shuffled_train_and_val_data.skip(num_val_samples)\n",
    "\n",
    "# Note: At this point, we now have scaled (0-1) data for test, val and train (10k, 6k, and 54k, respectively)\n",
    "\n",
    "# Setup batching for mini-batch gradient descent (Note: SGD is technically batch-size=1, though the term SGD is often used to describe Mini-Batch GD)\n",
    "# Note: Only batch on training data, as for val data we only forward-propogate (whereas we forward and backpropogatae for training data), AND also b/c for\n",
    "# each epoch, we'd rather calculate exact loss/accuracy for val (using all val data together), whereas for training data we take the average loss across batches.\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "val_data = val_data.batch(num_val_samples) # Only batch on training data, but model expects val and test data in batch form too. (i.e., only 1 batch)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# Extract inputs and targets, as we loaded the MNIST data as iterable and in a 2-tuple format (when we set as_supervised=True)\n",
    "val_inputs, val_targets = next(iter(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outlining the model ###\n",
    "\n",
    "# 4 layers: 784 (28x28x1 flattened img) -> 50 -> 50 -> 10 digits/classes. (Baseline hyperparameters)\n",
    "# Will later tune: NN width, depth, activation functions, etc.\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # Replace this line with .Input(shape=(28, 28, 1)) and .Flatten() layers?\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Note, the below model definition code may better conform with Keras best practices and make the architecture's input more explicit\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(28, 28, 1)), # Explicit Input layer\n",
    "        tf.keras.layers.Flatten(), # Note: Moved input from this line to the above input layer\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_size, activation='softmax') # Softmax usually used for output layer in classification problems\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose the optimizer, loss function, and optionally any metrics to calculate throughout training and testing ###\n",
    "\n",
    "# Optimizer: Adam, or adaptive moment estimation, is very common (combines adaptive learning rate + momentum)\n",
    "# Loss: Will choose one of the crossentropy options, since this is a classification problem\n",
    "#       - categorical_crossentropy expects that you've one-hot encoded the targets\n",
    "#       - sparse_categorical_crossentropy applies one-hot encoding\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.8858 - loss: 0.4112 - val_accuracy: 0.9403 - val_loss: 0.1980\n",
      "Epoch 2/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9468 - loss: 0.1827 - val_accuracy: 0.9567 - val_loss: 0.1448\n",
      "Epoch 3/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9585 - loss: 0.1382 - val_accuracy: 0.9625 - val_loss: 0.1168\n",
      "Epoch 4/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9667 - loss: 0.1107 - val_accuracy: 0.9680 - val_loss: 0.1044\n",
      "Epoch 5/5\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9706 - loss: 0.0974 - val_accuracy: 0.9727 - val_loss: 0.0906\n"
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "\n",
    "NUM_EPOCHS = 5 # Arbitrary - Should probably train more epochs, then stop training just before overfitting (i.e., val loss starts increasing)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(val_inputs, val_targets),\n",
    "    verbose=2 # Provide details at each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9654 - loss: 0.1185\n",
      "Test loss: 0.1185. Test accuracy: 96.54%.\n"
     ]
    }
   ],
   "source": [
    "### Test the model (in practice, the test data should really only be used at the very end, after hyperparameter tuning) ###\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f'Test loss: {test_loss:.4f}. Test accuracy: {test_accuracy*100:.2f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alowe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alowe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/90) val_accuracy=98.63% | train_accuracy=98.91% for width=32, layer_size_decrease=constant, depth=2, activation=relu with epochs=24 and training_time=0min 23sec.\n",
      "(2/90) val_accuracy=98.48% | train_accuracy=99.23% for width=32, layer_size_decrease=constant, depth=2, activation=elu with epochs=26 and training_time=0min 22sec.\n",
      "(3/90) val_accuracy=99.25% | train_accuracy=99.51% for width=32, layer_size_decrease=constant, depth=2, activation=tanh with epochs=34 and training_time=0min 29sec.\n",
      "(4/90) val_accuracy=97.97% | train_accuracy=98.43% for width=32, layer_size_decrease=constant, depth=3, activation=relu with epochs=15 and training_time=0min 13sec.\n",
      "(5/90) val_accuracy=98.33% | train_accuracy=98.71% for width=32, layer_size_decrease=constant, depth=3, activation=elu with epochs=16 and training_time=0min 14sec.\n",
      "(6/90) val_accuracy=99.42% | train_accuracy=99.59% for width=32, layer_size_decrease=constant, depth=3, activation=tanh with epochs=35 and training_time=0min 31sec.\n",
      "(7/90) val_accuracy=96.60% | train_accuracy=97.64% for width=32, layer_size_decrease=constant, depth=4, activation=relu with epochs=9 and training_time=0min 9sec.\n",
      "(8/90) val_accuracy=98.68% | train_accuracy=99.03% for width=32, layer_size_decrease=constant, depth=4, activation=elu with epochs=22 and training_time=0min 20sec.\n",
      "(9/90) val_accuracy=98.88% | train_accuracy=99.30% for width=32, layer_size_decrease=constant, depth=4, activation=tanh with epochs=27 and training_time=0min 25sec.\n",
      "(10/90) val_accuracy=98.50% | train_accuracy=98.77% for width=32, layer_size_decrease=half, depth=2, activation=relu with epochs=20 and training_time=0min 17sec.\n",
      "(11/90) val_accuracy=98.90% | train_accuracy=99.37% for width=32, layer_size_decrease=half, depth=2, activation=elu with epochs=27 and training_time=0min 23sec.\n",
      "(12/90) val_accuracy=98.52% | train_accuracy=99.22% for width=32, layer_size_decrease=half, depth=2, activation=tanh with epochs=22 and training_time=0min 19sec.\n",
      "(13/90) val_accuracy=97.73% | train_accuracy=98.11% for width=32, layer_size_decrease=half, depth=3, activation=relu with epochs=13 and training_time=0min 12sec.\n",
      "(14/90) val_accuracy=98.78% | train_accuracy=99.13% for width=32, layer_size_decrease=half, depth=3, activation=elu with epochs=22 and training_time=0min 20sec.\n",
      "(15/90) val_accuracy=98.30% | train_accuracy=99.30% for width=32, layer_size_decrease=half, depth=3, activation=tanh with epochs=25 and training_time=0min 23sec.\n",
      "(16/90) val_accuracy=98.12% | train_accuracy=98.62% for width=32, layer_size_decrease=half, depth=4, activation=relu with epochs=19 and training_time=0min 18sec.\n",
      "(17/90) val_accuracy=98.22% | train_accuracy=98.90% for width=32, layer_size_decrease=half, depth=4, activation=elu with epochs=19 and training_time=0min 18sec.\n",
      "(18/90) val_accuracy=98.88% | train_accuracy=99.37% for width=32, layer_size_decrease=half, depth=4, activation=tanh with epochs=30 and training_time=0min 28sec.\n",
      "(19/90) val_accuracy=98.47% | train_accuracy=99.09% for width=64, layer_size_decrease=constant, depth=2, activation=relu with epochs=13 and training_time=0min 13sec.\n",
      "(20/90) val_accuracy=99.32% | train_accuracy=99.55% for width=64, layer_size_decrease=constant, depth=2, activation=elu with epochs=19 and training_time=0min 19sec.\n",
      "(21/90) val_accuracy=99.77% | train_accuracy=99.77% for width=64, layer_size_decrease=constant, depth=2, activation=tanh with epochs=22 and training_time=0min 22sec.\n",
      "(22/90) val_accuracy=98.07% | train_accuracy=98.74% for width=64, layer_size_decrease=constant, depth=3, activation=relu with epochs=10 and training_time=0min 11sec.\n",
      "(23/90) val_accuracy=99.02% | train_accuracy=99.25% for width=64, layer_size_decrease=constant, depth=3, activation=elu with epochs=14 and training_time=0min 14sec.\n",
      "(24/90) val_accuracy=99.47% | train_accuracy=99.62% for width=64, layer_size_decrease=constant, depth=3, activation=tanh with epochs=20 and training_time=0min 21sec.\n",
      "(25/90) val_accuracy=99.05% | train_accuracy=99.25% for width=64, layer_size_decrease=constant, depth=4, activation=relu with epochs=17 and training_time=0min 18sec.\n",
      "(26/90) val_accuracy=99.45% | train_accuracy=99.53% for width=64, layer_size_decrease=constant, depth=4, activation=elu with epochs=27 and training_time=0min 29sec.\n",
      "(27/90) val_accuracy=99.25% | train_accuracy=99.38% for width=64, layer_size_decrease=constant, depth=4, activation=tanh with epochs=16 and training_time=0min 18sec.\n",
      "(28/90) val_accuracy=98.72% | train_accuracy=99.23% for width=64, layer_size_decrease=half, depth=2, activation=relu with epochs=13 and training_time=0min 13sec.\n",
      "(29/90) val_accuracy=99.55% | train_accuracy=99.70% for width=64, layer_size_decrease=half, depth=2, activation=elu with epochs=27 and training_time=0min 26sec.\n",
      "(30/90) val_accuracy=99.82% | train_accuracy=99.74% for width=64, layer_size_decrease=half, depth=2, activation=tanh with epochs=24 and training_time=0min 24sec.\n",
      "(31/90) val_accuracy=99.50% | train_accuracy=99.44% for width=64, layer_size_decrease=half, depth=3, activation=relu with epochs=23 and training_time=0min 23sec.\n",
      "(32/90) val_accuracy=99.27% | train_accuracy=99.50% for width=64, layer_size_decrease=half, depth=3, activation=elu with epochs=18 and training_time=0min 18sec.\n",
      "(33/90) val_accuracy=98.28% | train_accuracy=99.47% for width=64, layer_size_decrease=half, depth=3, activation=tanh with epochs=15 and training_time=0min 16sec.\n",
      "(34/90) val_accuracy=98.15% | train_accuracy=98.59% for width=64, layer_size_decrease=half, depth=4, activation=relu with epochs=9 and training_time=0min 10sec.\n",
      "(35/90) val_accuracy=99.20% | train_accuracy=99.31% for width=64, layer_size_decrease=half, depth=4, activation=elu with epochs=18 and training_time=0min 19sec.\n",
      "(36/90) val_accuracy=98.87% | train_accuracy=99.39% for width=64, layer_size_decrease=half, depth=4, activation=tanh with epochs=14 and training_time=0min 16sec.\n",
      "(37/90) val_accuracy=99.30% | train_accuracy=99.48% for width=128, layer_size_decrease=constant, depth=2, activation=relu with epochs=15 and training_time=0min 18sec.\n",
      "(38/90) val_accuracy=99.58% | train_accuracy=99.70% for width=128, layer_size_decrease=constant, depth=2, activation=elu with epochs=20 and training_time=0min 24sec.\n",
      "(39/90) val_accuracy=99.88% | train_accuracy=99.85% for width=128, layer_size_decrease=constant, depth=2, activation=tanh with epochs=23 and training_time=0min 28sec.\n",
      "(40/90) val_accuracy=98.53% | train_accuracy=98.90% for width=128, layer_size_decrease=constant, depth=3, activation=relu with epochs=7 and training_time=0min 9sec.\n",
      "(41/90) val_accuracy=99.43% | train_accuracy=99.59% for width=128, layer_size_decrease=constant, depth=3, activation=elu with epochs=22 and training_time=0min 28sec.\n",
      "(42/90) val_accuracy=99.53% | train_accuracy=99.52% for width=128, layer_size_decrease=constant, depth=3, activation=tanh with epochs=15 and training_time=0min 20sec.\n",
      "(43/90) val_accuracy=98.45% | train_accuracy=99.04% for width=128, layer_size_decrease=constant, depth=4, activation=relu with epochs=9 and training_time=0min 13sec.\n",
      "(44/90) val_accuracy=98.75% | train_accuracy=99.26% for width=128, layer_size_decrease=constant, depth=4, activation=elu with epochs=13 and training_time=0min 18sec.\n",
      "(45/90) val_accuracy=98.92% | train_accuracy=99.53% for width=128, layer_size_decrease=constant, depth=4, activation=tanh with epochs=15 and training_time=0min 22sec.\n",
      "(46/90) val_accuracy=99.48% | train_accuracy=99.59% for width=128, layer_size_decrease=half, depth=2, activation=relu with epochs=21 and training_time=0min 25sec.\n",
      "(47/90) val_accuracy=99.60% | train_accuracy=99.69% for width=128, layer_size_decrease=half, depth=2, activation=elu with epochs=17 and training_time=0min 21sec.\n",
      "(48/90) val_accuracy=99.87% | train_accuracy=99.82% for width=128, layer_size_decrease=half, depth=2, activation=tanh with epochs=19 and training_time=0min 24sec.\n",
      "(49/90) val_accuracy=98.88% | train_accuracy=99.35% for width=128, layer_size_decrease=half, depth=3, activation=relu with epochs=11 and training_time=0min 15sec.\n",
      "(50/90) val_accuracy=99.60% | train_accuracy=99.69% for width=128, layer_size_decrease=half, depth=3, activation=elu with epochs=22 and training_time=0min 29sec.\n",
      "(51/90) val_accuracy=99.23% | train_accuracy=99.59% for width=128, layer_size_decrease=half, depth=3, activation=tanh with epochs=24 and training_time=0min 32sec.\n",
      "(52/90) val_accuracy=99.23% | train_accuracy=99.44% for width=128, layer_size_decrease=half, depth=4, activation=relu with epochs=16 and training_time=0min 22sec.\n",
      "(53/90) val_accuracy=97.88% | train_accuracy=98.52% for width=128, layer_size_decrease=half, depth=4, activation=elu with epochs=7 and training_time=0min 10sec.\n",
      "(54/90) val_accuracy=99.23% | train_accuracy=99.61% for width=128, layer_size_decrease=half, depth=4, activation=tanh with epochs=16 and training_time=0min 24sec.\n",
      "(55/90) val_accuracy=99.53% | train_accuracy=99.68% for width=256, layer_size_decrease=constant, depth=2, activation=relu with epochs=14 and training_time=0min 24sec.\n",
      "(56/90) val_accuracy=99.60% | train_accuracy=99.64% for width=256, layer_size_decrease=constant, depth=2, activation=elu with epochs=20 and training_time=0min 33sec.\n",
      "(57/90) val_accuracy=99.72% | train_accuracy=99.67% for width=256, layer_size_decrease=constant, depth=2, activation=tanh with epochs=18 and training_time=0min 31sec.\n",
      "(58/90) val_accuracy=98.72% | train_accuracy=99.25% for width=256, layer_size_decrease=constant, depth=3, activation=relu with epochs=9 and training_time=0min 18sec.\n",
      "(59/90) val_accuracy=99.12% | train_accuracy=99.45% for width=256, layer_size_decrease=constant, depth=3, activation=elu with epochs=15 and training_time=0min 30sec.\n",
      "(60/90) val_accuracy=99.40% | train_accuracy=99.72% for width=256, layer_size_decrease=constant, depth=3, activation=tanh with epochs=18 and training_time=0min 38sec.\n",
      "(61/90) val_accuracy=98.35% | train_accuracy=98.92% for width=256, layer_size_decrease=constant, depth=4, activation=relu with epochs=7 and training_time=0min 16sec.\n",
      "(62/90) val_accuracy=98.33% | train_accuracy=98.61% for width=256, layer_size_decrease=constant, depth=4, activation=elu with epochs=8 and training_time=0min 19sec.\n",
      "(63/90) val_accuracy=99.07% | train_accuracy=99.37% for width=256, layer_size_decrease=constant, depth=4, activation=tanh with epochs=12 and training_time=0min 30sec.\n",
      "(64/90) val_accuracy=99.12% | train_accuracy=99.66% for width=256, layer_size_decrease=half, depth=2, activation=relu with epochs=13 and training_time=0min 22sec.\n",
      "(65/90) val_accuracy=99.22% | train_accuracy=99.37% for width=256, layer_size_decrease=half, depth=2, activation=elu with epochs=14 and training_time=0min 24sec.\n",
      "(66/90) val_accuracy=99.57% | train_accuracy=99.76% for width=256, layer_size_decrease=half, depth=2, activation=tanh with epochs=13 and training_time=0min 23sec.\n",
      "(67/90) val_accuracy=98.85% | train_accuracy=99.29% for width=256, layer_size_decrease=half, depth=3, activation=relu with epochs=10 and training_time=0min 20sec.\n",
      "(68/90) val_accuracy=99.00% | train_accuracy=99.35% for width=256, layer_size_decrease=half, depth=3, activation=elu with epochs=16 and training_time=0min 32sec.\n",
      "(69/90) val_accuracy=99.52% | train_accuracy=99.66% for width=256, layer_size_decrease=half, depth=3, activation=tanh with epochs=22 and training_time=0min 46sec.\n",
      "(70/90) val_accuracy=99.45% | train_accuracy=99.60% for width=256, layer_size_decrease=half, depth=4, activation=relu with epochs=17 and training_time=0min 39sec.\n",
      "(71/90) val_accuracy=98.78% | train_accuracy=98.96% for width=256, layer_size_decrease=half, depth=4, activation=elu with epochs=12 and training_time=0min 28sec.\n",
      "(72/90) val_accuracy=99.45% | train_accuracy=99.59% for width=256, layer_size_decrease=half, depth=4, activation=tanh with epochs=16 and training_time=0min 39sec.\n",
      "(73/90) val_accuracy=99.02% | train_accuracy=99.32% for width=512, layer_size_decrease=constant, depth=2, activation=relu with epochs=8 and training_time=0min 23sec.\n",
      "(74/90) val_accuracy=99.15% | train_accuracy=99.21% for width=512, layer_size_decrease=constant, depth=2, activation=elu with epochs=13 and training_time=0min 37sec.\n",
      "(75/90) val_accuracy=99.42% | train_accuracy=99.57% for width=512, layer_size_decrease=constant, depth=2, activation=tanh with epochs=14 and training_time=0min 39sec.\n",
      "(76/90) val_accuracy=97.80% | train_accuracy=99.14% for width=512, layer_size_decrease=constant, depth=3, activation=relu with epochs=6 and training_time=0min 23sec.\n",
      "(77/90) val_accuracy=98.62% | train_accuracy=98.80% for width=512, layer_size_decrease=constant, depth=3, activation=elu with epochs=12 and training_time=0min 45sec.\n",
      "(78/90) val_accuracy=98.57% | train_accuracy=98.90% for width=512, layer_size_decrease=constant, depth=3, activation=tanh with epochs=8 and training_time=0min 30sec.\n",
      "(79/90) val_accuracy=98.65% | train_accuracy=98.90% for width=512, layer_size_decrease=constant, depth=4, activation=relu with epochs=6 and training_time=0min 28sec.\n",
      "(80/90) val_accuracy=96.40% | train_accuracy=97.50% for width=512, layer_size_decrease=constant, depth=4, activation=elu with epochs=5 and training_time=0min 23sec.\n",
      "(81/90) val_accuracy=98.72% | train_accuracy=98.90% for width=512, layer_size_decrease=constant, depth=4, activation=tanh with epochs=11 and training_time=0min 51sec.\n",
      "(82/90) val_accuracy=99.55% | train_accuracy=99.58% for width=512, layer_size_decrease=half, depth=2, activation=relu with epochs=16 and training_time=0min 45sec.\n",
      "(83/90) val_accuracy=99.47% | train_accuracy=99.56% for width=512, layer_size_decrease=half, depth=2, activation=elu with epochs=21 and training_time=0min 59sec.\n",
      "(84/90) val_accuracy=99.38% | train_accuracy=99.45% for width=512, layer_size_decrease=half, depth=2, activation=tanh with epochs=10 and training_time=0min 29sec.\n",
      "(85/90) val_accuracy=99.05% | train_accuracy=99.44% for width=512, layer_size_decrease=half, depth=3, activation=relu with epochs=11 and training_time=0min 41sec.\n",
      "(86/90) val_accuracy=98.48% | train_accuracy=98.59% for width=512, layer_size_decrease=half, depth=3, activation=elu with epochs=9 and training_time=0min 34sec.\n",
      "(87/90) val_accuracy=98.75% | train_accuracy=99.25% for width=512, layer_size_decrease=half, depth=3, activation=tanh with epochs=10 and training_time=0min 37sec.\n",
      "(88/90) val_accuracy=98.75% | train_accuracy=99.36% for width=512, layer_size_decrease=half, depth=4, activation=relu with epochs=10 and training_time=0min 46sec.\n",
      "(89/90) val_accuracy=98.43% | train_accuracy=98.36% for width=512, layer_size_decrease=half, depth=4, activation=elu with epochs=10 and training_time=0min 45sec.\n",
      "(90/90) val_accuracy=98.90% | train_accuracy=98.92% for width=512, layer_size_decrease=half, depth=4, activation=tanh with epochs=11 and training_time=0min 51sec.\n",
      "----------------------------------------------------------------------\n",
      "best_val_loss=0.0055 | best_val_accuracy=0.9988 | best_train_accuracy=0.9985\n",
      "Best model available at 'best_model'\n",
      "Best Hyperparameters:\n",
      "{'hidden_layer_size': 128, 'hidden_layer_size_variation': 'constant', 'hidden_layer_depth': 2, 'activation_function': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter tuning (and also adding Early Stopping) ###\n",
    "\n",
    "# Note 1: Manually implementing a tuning process very close to GridSearchCV, but in practice using an API\n",
    "#         like the Keras Tuner's Bayesian Hyperparameter Optimization is likely preferred.\n",
    "# Note 2: The \"Pre-process the data\" and earlier cells must be executed prior to this cell.\n",
    "\n",
    "import time\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "hidden_layer_sizes = [32, 64, 128, 256, 512]\n",
    "hidden_layer_size_decreases = ['constant', 'half']\n",
    "hidden_layer_depths = [2, 3, 4]\n",
    "activation_functions = ['relu', 'elu', 'tanh']\n",
    "\n",
    "total_num_scenarios = (\n",
    "    len(hidden_layer_sizes) *\n",
    "    len(hidden_layer_size_decreases) * \n",
    "    len(hidden_layer_depths) *\n",
    "    len(activation_functions)\n",
    ")\n",
    "curr_scenario_num = 0 # Increment for each loop\n",
    "\n",
    "# Initialize best model and parameters to store info on best during loops\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0\n",
    "best_train_accuracy = 0\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for hidden_layer_size_decrease in hidden_layer_size_decreases:\n",
    "        for hidden_layer_depth in hidden_layer_depths:\n",
    "            for activation_function in activation_functions:\n",
    "\n",
    "                loop_start_time = time.time()\n",
    "\n",
    "                # Reset the global state, releasing memory\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                # Create the EarlyStopping callback\n",
    "                early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=2, # Number of epochs to wait for improvement before stopping\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "\n",
    "                # Build the model by adding each layer, w/ dynamic # of hidden layers\n",
    "                curr_model = tf.keras.Sequential()\n",
    "                curr_model.add(tf.keras.layers.Input(shape=(28, 28, 1)))\n",
    "                curr_model.add(tf.keras.layers.Flatten())\n",
    "                for i in range(hidden_layer_depth):\n",
    "                    if hidden_layer_size_decrease == 'constant': # Constant size\n",
    "                        current_layer_size = hidden_layer_size\n",
    "                    elif hidden_layer_size_decrease == 'decrease_half': # Each hidden layer reduces by half\n",
    "                        current_layer_size = hidden_layer_size // (2 ** i)\n",
    "                    curr_model.add(tf.keras.layers.Dense(current_layer_size, activation=activation_function))\n",
    "                curr_model.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
    "\n",
    "                # Compile model optimizer and loss then fit/train the model\n",
    "                curr_model.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "                curr_history = curr_model.fit(\n",
    "                    train_data,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(val_inputs, val_targets),\n",
    "                    verbose=0,\n",
    "                    callbacks=[early_stopping]\n",
    "                )\n",
    "\n",
    "                # Check if current model is best so far, and if so, update best model w/ current one. Print training info.\n",
    "                val_accuracy = curr_history.history['val_accuracy'][-1]\n",
    "                train_accuracy = curr_history.history['accuracy'][-1]\n",
    "                epoch_stop = len(curr_history.history['val_loss'])\n",
    "                curr_scenario_num += 1\n",
    "                curr_scenario_elapsed_time = time.time() - loop_start_time\n",
    "                print(\n",
    "                    f'({curr_scenario_num}/{total_num_scenarios}) '\n",
    "                    f'val_accuracy={val_accuracy*100:.2f}% | train_accuracy={train_accuracy*100:.2f}% for '\n",
    "                    f'width={hidden_layer_size}, '\n",
    "                    f'layer_size_decrease={hidden_layer_size_decrease}, '\n",
    "                    f'depth={hidden_layer_depth}, '\n",
    "                    f'activation={activation_function} with '\n",
    "                    f'epochs={epoch_stop} and '\n",
    "                    f'training_time={int(curr_scenario_elapsed_time // 60)}min {int(curr_scenario_elapsed_time % 60)}sec.'\n",
    "                )\n",
    "                if epoch_stop == NUM_EPOCHS:\n",
    "                    print(f'Warning: Model reached epoch #{NUM_EPOCHS} for the above accuracy. Consider re-running with higher NUM_EPOCHS.')\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_loss = curr_history.history['val_loss'][-1]\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_train_accuracy = train_accuracy\n",
    "                    best_model = curr_model\n",
    "                    best_params = {\n",
    "                        'hidden_layer_size': hidden_layer_size,\n",
    "                        'hidden_layer_size_variation': hidden_layer_size_decrease,\n",
    "                        'hidden_layer_depth': hidden_layer_depth,\n",
    "                        'activation_function': activation_function\n",
    "                    }\n",
    "\n",
    "\n",
    "# Print info about best model\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'{best_val_loss=:.4f} | {best_val_accuracy=:.4f} | {best_train_accuracy=:.4f}')\n",
    "print(\"Best model available at 'best_model'\")\n",
    "print('Best Hyperparameters:')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9801 - loss: 0.0849\n",
      "Test loss: 0.0849 | Test accuracy: 98.01%\n",
      "best_val_loss=0.0055 | best_val_accuracy=0.9988 | best_train_accuracy=0.9985\n",
      "Best Hyperparameters:\n",
      "{'hidden_layer_size': 128, 'hidden_layer_size_variation': 'constant', 'hidden_layer_depth': 2, 'activation_function': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "### Test the model ###\n",
    "\n",
    "test_loss, test_accuracy = best_model.evaluate(test_data)\n",
    "print(f'Test loss: {test_loss:.4f} | Test accuracy: {test_accuracy*100:.2f}%')\n",
    "print(f'{best_val_loss=:.4f} | {best_val_accuracy=:.4f} | {best_train_accuracy=:.4f}')\n",
    "print('Best Hyperparameters:')\n",
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
